{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/Ábel/Desktop/Prog_nyelvek/ProjectWork/Algorithms/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing request for news requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime for giving start and end date parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For serlization (writing and reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"58dc66dc611848a694e6fdd04c66917a\"  # Replace with your actual API key\n",
    "news_api = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to fetch and store the articles (Sadly, this method is deprecated now there is a limit in the dev. requests ( max of 100 results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_store_articles_old(keyword, start_date, end_date, output_file):\n",
    "\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    response = news_api.get_everything(\n",
    "        q=keyword,\n",
    "        from_param=start_date_str,\n",
    "        to=end_date_str,\n",
    "        sort_by='publishedAt' \n",
    "    )\n",
    "\n",
    "    if response[\"status\"] == \"ok\":\n",
    "        if response[\"totalResults\"] > 0:\n",
    "            articles_list = []\n",
    "            for article in response[\"articles\"]:\n",
    "                article_details = {\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"description\": article[\"description\"],\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"published_at\": article[\"publishedAt\"]\n",
    "                }\n",
    "                articles_list.append(article_details)\n",
    "\n",
    "            with open(output_file, 'w') as json_file:\n",
    "                json.dump(articles_list, json_file, indent=2)\n",
    "\n",
    "            print(f\"Articles successfully stored in {output_file}\")\n",
    "        else:\n",
    "            print(\"No articles found.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch articles. Error: {response.get('message', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to fetch and store the articles (UpToDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_store_articles(keyword, start_date, end_date, output_file, interval_days=1):\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    articles_list = []\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        \n",
    "        interval_end = min(current_date + timedelta(days=interval_days - 1), end_date)\n",
    "\n",
    "        response = news_api.get_everything(\n",
    "            q=keyword,\n",
    "            from_param=current_date.strftime(\"%Y-%m-%d\"),\n",
    "            to=interval_end.strftime(\"%Y-%m-%d\"),\n",
    "            sort_by='publishedAt',\n",
    "            page_size=100\n",
    "        )\n",
    "\n",
    "        if response[\"status\"] == \"ok\":\n",
    "            if response[\"totalResults\"] > 0:\n",
    "                for article in response[\"articles\"]:\n",
    "                    article_details = {\n",
    "                        \"title\": article[\"title\"],\n",
    "                        \"description\": article[\"description\"],\n",
    "                        \"url\": article[\"url\"],\n",
    "                        \"published_at\": article[\"publishedAt\"]\n",
    "                    }\n",
    "                    articles_list.append(article_details)\n",
    "            else:\n",
    "                print(f\"No articles found from {current_date.strftime('%Y-%m-%d')} to {interval_end.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch articles for {current_date.strftime('%Y-%m-%d')} to {interval_end.strftime('%Y-%m-%d')}. Error: {response.get('message', 'Unknown error')}\")\n",
    "\n",
    "        #Next interval\n",
    "        current_date += timedelta(days=interval_days)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(articles_list, json_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Articles successfully stored in {output_file}. Total articles fetched: {len(articles_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading setup parametes from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameters.txt', 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = lines[0].strip()\n",
    "start_date = datetime.strptime(start_date, '%Y-%m-%d').replace(hour=0, minute=0, second=0)\n",
    "end_date = lines[1].strip()\n",
    "end_date = datetime.strptime(end_date, '%Y-%m-%d').replace(hour=0, minute=0, second=0)\n",
    "keyword = lines[2].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_format = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_date_format = end_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"Data/BaseJson/all_articles_{keyword}_{start_date_format}_{end_date_format}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to fetch and store the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles successfully stored in Data/BaseJson/all_articles_Berkshire Hathaway_2024-05-10_2024-05-10.json. Total articles fetched: 35\n"
     ]
    }
   ],
   "source": [
    "fetch_and_store_articles(keyword, start_date, end_date, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract text from a given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Json for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_path = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Json for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_path = f'Data/FullJson/fullout_all_articles_{keyword}_{start_date_format}_{end_date_format}.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_json_path, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each entry and store in a new JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching content from https://www.kpbs.org/podcasts/kpbs-roundtable/student-journalists-role-in-telling-story-of-ucsd-encampment: 403 Client Error: Forbidden for url: https://www.kpbs.org/podcasts/kpbs-roundtable/student-journalists-role-in-telling-story-of-ucsd-encampment\n",
      "Error fetching content from https://www.fool.com.au/2024/05/11/top-asx-shares-to-buy-for-your-superannuation-fund-in-may-2024/: 403 Client Error: Forbidden for url: https://www.fool.com.au/2024/05/11/top-asx-shares-to-buy-for-your-superannuation-fund-in-may-2024/\n",
      "Error fetching content from https://www.marketscreener.com/quote/stock/JONES-LANG-LASALLE-INCORP-13180/news/Jones-Lang-LaSalle-Incorporated-Johns-Manville-signs-new-lease-to-keep-world-headquarters-in-Denve-46693274/: 403 Client Error: Forbidden. for url: https://www.marketscreener.com/quote/stock/JONES-LANG-LASALLE-INCORP-13180/news/Jones-Lang-LaSalle-Incorporated-Johns-Manville-signs-new-lease-to-keep-world-headquarters-in-Denve-46693274/\n",
      "Error fetching content from https://removed.com: 436 Client Error: status code 436 for url: https://removed.com/\n",
      "Error fetching content from http://www.tikalon.com/blog/blog.php?article=2024/IBM_100: 403 Client Error: Forbidden for url: http://www.tikalon.com/blog/blog.php?article=2024/IBM_100\n",
      "Error fetching content from https://www.investing.com/news/economy/columnforget-market-angst-just-kiss-and-make-up-mike-dolan-3433580: 403 Client Error: Forbidden for url: https://www.investing.com/news/economy/columnforget-market-angst-just-kiss-and-make-up-mike-dolan-3433580\n",
      "Error fetching content from https://www.marketscreener.com/quote/stock/BERKSHIRE-HATHAWAY-INC-11915/news/Forget-market-angst-just-KISS-and-make-up-Mike-Dolan-46686708/: 403 Client Error: Forbidden. for url: https://www.marketscreener.com/quote/stock/BERKSHIRE-HATHAWAY-INC-11915/news/Forget-market-angst-just-KISS-and-make-up-Mike-Dolan-46686708/\n"
     ]
    }
   ],
   "source": [
    "output_data = []\n",
    "for entry in json_data:\n",
    "    title = entry[\"title\"]\n",
    "    description = entry[\"description\"]\n",
    "    published_at = entry[\"published_at\"]\n",
    "\n",
    "    text_content = extract_text_from_url(entry[\"url\"])\n",
    "\n",
    "    new_entry = {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"published_at\": published_at,\n",
    "        \"content\": text_content\n",
    "    }\n",
    "    \n",
    "    output_data.append(new_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(output_data, output_file, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Results stored in Data/FullJson/fullout_all_articles_Berkshire Hathaway_2024-05-10_2024-05-10.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extraction complete. Results stored in {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing csv for exporting out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\ábel\\desktop\\prog_nyelvek\\projectwork\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\ábel\\desktop\\prog_nyelvek\\projectwork\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ábel\\desktop\\prog_nyelvek\\projectwork\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ábel\\desktop\\prog_nyelvek\\projectwork\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ábel\\desktop\\prog_nyelvek\\projectwork\\lib\\site-packages (from nltk>=3.1->textblob) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ábel\\desktop\\prog_nyelvek\\projectwork\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ábel\\desktop\\prog_nyelvek\\projectwork\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing textblob for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load JSON data with Unicode decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate sentiment score between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_normalized(text):\n",
    "    if text is None:\n",
    "        text = \"\"  # Default to empty string if input is None\n",
    "    analysis = TextBlob(text)\n",
    "    sentiment_score = analysis.sentiment.polarity  # No need to scale to 10\n",
    "    normalized_score = (sentiment_score + 1) / 2  # Normalize between 0 and 1\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each entry in the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    title = entry['title']\n",
    "    content = entry['content']\n",
    "    description = entry['description']\n",
    "    date = entry['published_at']\n",
    "\n",
    "    if title == \"[Removed]\":\n",
    "        continue\n",
    "\n",
    "    if content is not None:\n",
    "        title_sentiment = calculate_sentiment_normalized(title)\n",
    "        content_sentiment = calculate_sentiment_normalized(content)\n",
    "\n",
    "        if description is not None:\n",
    "            description_sentiment = calculate_sentiment_normalized(description)\n",
    "        else:\n",
    "            description_sentiment = 0.5\n",
    "\n",
    "        csv_data.append({\n",
    "            'date': date,\n",
    "            'title_sentiment': title_sentiment,\n",
    "            'content_sentiment': content_sentiment,\n",
    "            'description_sentiment': description_sentiment\n",
    "        })\n",
    "    else:\n",
    "\n",
    "        csv_data.append({\n",
    "            'date': date,\n",
    "            'title_sentiment': 0.5,\n",
    "            'content_sentiment': 0.5,  \n",
    "            'description_sentiment': 0.5  \n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv file output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = f'Data/OutputSent/output_sentiments_normalized_{keyword}_{start_date_format}_{end_date_format}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Csv write back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['date', 'title_sentiment', 'content_sentiment', 'description_sentiment']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for entry in csv_data:\n",
    "        writer.writerow({\n",
    "            'date': entry['date'],\n",
    "            'title_sentiment': entry['title_sentiment'],\n",
    "            'content_sentiment': entry['content_sentiment'],\n",
    "            'description_sentiment': entry['description_sentiment']\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Pandas for creating dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the 'date' column to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by date and calculate the average for each sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = df.groupby('date').mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the resulting DataFrame to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=f'Data/AverageSent/average_sentiments_{keyword}_{start_date_format}_{end_date_format}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiments saved to Data/AverageSent/average_sentiments_Berkshire Hathaway_2024-05-10_2024-05-10.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average sentiments saved to {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectWork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
